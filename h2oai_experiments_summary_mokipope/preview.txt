Experiment: 15802a80-a4ad-11e9-afed-0242ac110004

ACCURACY [6/10]:
- Training data size: *30,000 rows, 25 cols*
- Feature evolution: *[XGBoostGBM, LightGBM]*, *1/3 validation split*
- Final pipeline: *Ensemble (10 models), 5-fold CV*

TIME [3/10]:
- Feature evolution: *4 individuals*, up to *62 iterations*
- Early stopping: After *5* iterations of no improvement

INTERPRETABILITY [6/10]:
- Feature pre-pruning strategy: FS
- XGBoost Monotonicity constraints: disabled
- Feature engineering search space (where applicable): [WeightOfEvidence, IsHoliday, ClusterId, NumToCatWoE, Original, Dates, Text, CVTargetEncode, Frequent, NumToCatTE, TextLinModel, CVCatNumEncode, NumCatTE, ClusterTE, Interactions]

[XGBoostGBM, LightGBM] models to train:
- Model and feature tuning: *32*
- Feature evolution: *64*
- Final pipeline: *10*

Estimated runtime: *minutes*
